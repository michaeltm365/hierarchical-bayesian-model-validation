{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Loading Data and CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'approx1e5-pastis-wavelet-gray' # Dataset Format: size-name-transform-channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = Path(git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "path_dict = dict(zip(['size', 'name', 'transform', 'channel'], DATA_NAME.split(\"-\")))\n",
    "CWD = os.path.join(ROOT_DIR, os.path.join(\"results\", \"case-studies\", path_dict['name'], path_dict['transform'], path_dict['size'], path_dict['channel']))\n",
    "assert CWD == os.getcwd()\n",
    "Path(os.path.join(CWD, \"CSVs\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"plots\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"cache\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"groupCDFs\")).mkdir(exist_ok=True)\n",
    "\n",
    "GROUP = 'layer' if path_dict['transform'] == 'wavelet' else ('band' if path_dict['transform'] == 'fourier' else 'error')\n",
    "RERUN = False\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(ROOT_DIR, \"utilities\"))\n",
    "from testing import * # If MATLAB is not installed, open utilities and set to False\n",
    "from plotting import *\n",
    "os.chdir(CWD)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data_map = pd.read_pickle(os.path.join(ROOT_DIR, \"transformed-data\", f'{DATA_NAME}.pickle'))\n",
    "group_total_samples = pd.read_pickle(os.path.join(ROOT_DIR, \"transformed-data\", f'{DATA_NAME}-size.pickle'))\n",
    "\n",
    "if path_dict['transform'] == 'fourier':\n",
    "    GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)[::3]\n",
    "elif path_dict['transform'] == 'wavelet':\n",
    "    GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)\n",
    "\n",
    "cdfs_dir = os.path.join(ROOT_DIR, \"results\", \"CDFs\")\n",
    "cdfs_list = [os.path.join(cdfs_dir, i) for i in os.listdir(cdfs_dir)]\n",
    "all_cdfs = combine_pickles(cdfs_list[0])\n",
    "for cdf_dir in cdfs_list[1:]:\n",
    "    all_cdfs = all_cdfs | combine_pickles(cdf_dir)\n",
    "    \n",
    "# group_data_map = {g : group_data_map[g][::1000] for g in GROUPS} # For quick testing purposes\n",
    "group_total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "all_cdfs_df = create_kurt_var_ksstat_df(all_cdfs)\n",
    "coarse_cdf_df = all_cdfs_df[(np.round(all_cdfs_df['r'], 0) == all_cdfs_df['r']) & (np.round(all_cdfs_df['eta'], 0) == all_cdfs_df['eta'])]\n",
    "var_values_dict = dict()\n",
    "kurt_values_dict = dict()\n",
    "master_df = pd.DataFrame(columns=[GROUP]).set_index(GROUP)\n",
    "temp_cdf = all_cdfs_df\n",
    "all_cdfs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the Variance and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = int(1e5)\n",
    "bootstrap_size = int(1e4)\n",
    "ci = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_path = Path(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "master_df_var_kurt_path = Path(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "\n",
    "if RERUN or not master_df_var_kurt_path.exists():\n",
    "    for group in GROUPS:\n",
    "        print(f'{GROUP.capitalize()} {group}')\n",
    "        obs_var, var_lower, var_upper, var_values_dict[group] = bootstrap_metric(group_data_map[group], \n",
    "                                                                                n_bootstrap=n_bootstrap, \n",
    "                                                                                bootstrap_size=min(group_data_map[group].size, bootstrap_size), \n",
    "                                                                                metric= np.var, \n",
    "                                                                                ci=ci)\n",
    "        obs_kurt, kurt_lower, kurt_upper, kurt_values_dict[group] = bootstrap_metric(group_data_map[group], \n",
    "                                                                                    n_bootstrap=n_bootstrap, \n",
    "                                                                                    bootstrap_size=min(group_data_map[group].size, bootstrap_size), \n",
    "                                                                                    metric= stats.kurtosis, ci=ci)  \n",
    "        master_df.loc[group, 'obs_var'], master_df.loc[group, 'var_lower'], master_df.loc[group, 'var_upper'] = obs_var, var_lower, var_upper\n",
    "        master_df.loc[group, 'obs_kurt'], master_df.loc[group, 'kurt_lower'], master_df.loc[group, 'kurt_upper'] = obs_kurt, kurt_lower, kurt_upper\n",
    "        master_df.loc[group, 'total_samples'] = group_total_samples[group]\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "\n",
    "master_df = pd.read_csv(master_df_var_kurt_path, index_col=GROUP)\n",
    "var_kurt_df = pd.read_csv(bootstrap_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Grid Search and Hypothesis Test Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_combo_path = Path(os.path.join(CWD, \"CSVs\", \"master_df_combo.csv\"))\n",
    "\n",
    "if RERUN or not master_df_combo_path.exists():\n",
    "    sorted_params = sorted(all_cdfs)\n",
    "    group_cdf_df_dict = dict()\n",
    "    ksstats_dict = dict()\n",
    "\n",
    "    for i, group in enumerate(GROUPS):\n",
    "        print(f\"####\\n{GROUP.capitalize()} \", group)\n",
    "        sample = group_data_map[group]\n",
    "        group_cdf_df = all_cdfs_df.copy()\n",
    "        \n",
    "        group_cdf_df['scale'] = np.clip(master_df.loc[group,'obs_var'] / group_cdf_df['variance'], 0, 1e5)\n",
    "        ksstats, initial_param, min_stat = gridsearch(sample, all_cdfs, debug=True, scales=group_cdf_df['scale'])\n",
    "        initial_scale = group_cdf_df.loc[(group_cdf_df[\"r\"] ==  initial_param[0]) & (group_cdf_df[\"eta\"] ==  initial_param[1])][\"scale\"].iloc[0]\n",
    "        master_df.loc[group, 'initial_r'], master_df.loc[group, 'initial_eta'] = initial_param\n",
    "        master_df.loc[group, 'initial_scale'] = initial_scale\n",
    "        print(f\"Number of samples: {sample.size}, Without approximation : {master_df.loc[group, 'total_samples']}\")\n",
    "        master_df.loc[group, 'kstest_stat_initial'] = min_stat\n",
    "        cutoff = stats.kstwo(n=master_df.loc[group, 'total_samples']).isf(0.05)\n",
    "        master_df.loc[group, 'kstest_stat_cutoff_0.05'] = cutoff\n",
    "\n",
    "        group_cdf_df['variance'] = group_cdf_df['variance'] * group_cdf_df['scale']\n",
    "        group_cdf_df['kurtosis'] = group_cdf_df['kurtosis'] * group_cdf_df['scale']\n",
    "\n",
    "        group_cdf_df = add_tests_to_df(cdfs_df = group_cdf_df, group = group, var_kurt_df = master_df, ksstats = ksstats)\n",
    "        group_cdf_df_dict[group] = group_cdf_df\n",
    "        cols = ['pass_var', 'pass_kstest', 'pass_kurt']\n",
    "\n",
    "        fig = combo_test_plot(group_cdf_df_dict[group], cols, \n",
    "                            plot_name=f\"{GROUP.capitalize()} {group}: {', '.join([col[5:].capitalize() for col in cols])}\", \n",
    "                            target_var = None,\n",
    "                            best_param = initial_param)\n",
    "        \n",
    "        fig.figure.savefig(os.path.join(CWD, \"plots\", f\"full_grid_search_combo_plot_layer{group}.jpg\"), bbox_inches = 'tight', dpi=600)\n",
    "\n",
    "        # Optional: \n",
    "        # Create plots of bootstrapped variance and kurtosis for varying confidence intervals\n",
    "        # fig_var = create_ci_scatter_plot(group_cdf_df_dict[group], var_values_dict, metric='variance', group=group)\n",
    "        # fig_kurt = create_ci_scatter_plot(group_cdf_df_dict[group], kurt_values_dict, metric='kurtosis', group=group)\n",
    "\n",
    "        # fig_var.savefig(os.path.join(CWD, \"plots\", f\"ci_scatter_variance_{GROUP}_{group}_bootstrap{n_bootstrap}.jpg\"), bbox_inches='tight')\n",
    "        # plt.close(fig_var)\n",
    "        # fig_kurt.savefig(os.path.join(CWD, \"plots\", f\"ci_scatter_variance_{GROUP}_{group}_bootstrap{n_bootstrap}.jpg\"), bbox_inches='tight')\n",
    "        # plt.close(fig_kurt)\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", \"master_df_combo.csv\"))\n",
    "\n",
    "master_df = pd.read_csv(master_df_combo_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search over $\\eta=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_eta0_path = Path(os.path.join(CWD, \"CSVs\", \"master_df_eta0.csv\"))\n",
    "\n",
    "if RERUN or not master_df_eta0_path.exists():\n",
    "    best_params_eta0 = []\n",
    "    for i, group in enumerate(GROUPS):\n",
    "        if master_df.loc[group, 'initial_eta'] != 0:\n",
    "            sample = group_data_map[group]\n",
    "            group_cdf = all_cdfs_df[all_cdfs_df['eta'] == 0]\n",
    "            cdfs_dict = {i[0]:i[1] for i in group_cdf['(r,eta),cdf']}\n",
    "            ksstats, best_param_eta0, kstest_stat_eta0 = gridsearch(sample, cdfs_dict, debug=True, scales=group_cdf_df['scale'])\n",
    "            best_scale_eta0 = group_cdf_df.loc[(group_cdf_df[\"r\"] ==  best_param_eta0[0]) & (group_cdf_df[\"eta\"] ==  best_param_eta0[1])][\"scale\"].iloc[0]\n",
    "            master_df.loc[group, 'best_r_eta0'] = best_param_eta0[0]\n",
    "            master_df.loc[group, 'best_scale_eta0'] = best_scale_eta0\n",
    "            master_df.loc[group, 'kstest_stat_eta0'] = kstest_stat_eta0\n",
    "        else:\n",
    "            master_df.loc[group, 'kstest_stat_eta0'] = master_df.loc[group, 'kstest_stat_initial']\n",
    "            master_df.loc[group, 'best_r_eta0'] = master_df.loc[group, 'initial_r']\n",
    "            master_df.loc[group, 'best_scale_eta0'] = master_df.loc[group, 'initial_scale']\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", \"master_df_eta0.csv\"))\n",
    "\n",
    "master_df = pd.read_csv(master_df_eta0_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add_cdfs(r_range, eta_range, dir, folder_name = '', n_samples = 500, tail_bound = 0.01, tail_percent = 0.1, enforce_assert=True, return_assert = False, debug=False):\n",
    "\n",
    "    if folder_name == '':\n",
    "        folder_name = f'r{round_to_sigfigs(min(r_range))}to{round_to_sigfigs(max(r_range))}_eta{round_to_sigfigs(min(eta_range))}to{round_to_sigfigs(max(eta_range))}'\n",
    "\n",
    "    FOLDER_PATH = os.path.join(dir, folder_name)\n",
    "\n",
    "    if os.path.isdir(FOLDER_PATH):\n",
    "        cdfs_completed = combine_pickles(FOLDER_PATH)\n",
    "        if debug:\n",
    "            print(\"CDFs completed:\", len(cdfs_completed))\n",
    "    else:\n",
    "        Path(os.path.join(os.getcwd(), FOLDER_PATH)).mkdir()\n",
    "        cdfs_completed = dict()\n",
    "\n",
    "    n = len(r_range)*len(eta_range)\n",
    "    finished = len(cdfs_completed)\n",
    "    cnt = len(cdfs_completed)\n",
    "    for r in r_range:\n",
    "        r_cdf = dict()\n",
    "        r = round_to_sigfigs(r)\n",
    "        for eta in eta_range:\n",
    "            eta = round_to_sigfigs(eta)\n",
    "            if ((r, eta) in cdfs_completed) and cdfs_completed[(r, eta)]:\n",
    "                continue\n",
    "            cnt += 1\n",
    "            if debug:\n",
    "                print(f'{(r, eta)}, {cnt} of {n + finished}')\n",
    "            if cnt % 50 == 0:\n",
    "                print(f'{(r, eta)}, {cnt} of {n + finished}')\n",
    "\n",
    "            computed_cdf = compute_prior_cdf(r = r, eta = eta, method = 'gamma_cdf', n_samples = n_samples, tail_percent = tail_percent, tail_bound = tail_bound, \n",
    "                                             enforce_assert=enforce_assert, return_assert=return_assert, debug=debug)\n",
    "            r_cdf[(r, eta)] = computed_cdf\n",
    "        if r_cdf:\n",
    "            sorted_r_cdf = [i[1] for i in sorted(r_cdf)]\n",
    "            min_eta, max_eta = round_to_sigfigs(min(sorted_r_cdf), 6), round_to_sigfigs(max(sorted_r_cdf), 6)\n",
    "            pkl_path = os.path.join(FOLDER_PATH, f'r{r}_eta{min_eta}to{max_eta}.pickle')\n",
    "            pd.to_pickle(r_cdf, pkl_path)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"Skipped {r} entirely\")\n",
    "\n",
    "    if debug:\n",
    "        print(f'You can find the CDFs here: {os.path.join(os.getcwd(), FOLDER_PATH)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_optimized_path = Path(os.path.join(CWD, \"CSVs\", 'master_df_optimized.csv'))\n",
    "rEtaKsstats_dict_path = Path(os.path.join(CWD, \"cache\", \"rEtaKsstats_dict.pickle\"))\n",
    "\n",
    "\n",
    "SKIP_OPTIMIZE_STEP = False\n",
    "NUM_ITERS = dict(zip(GROUPS, [1]*len(GROUPS)))\n",
    "NUM_SAMPLES_OPTIMIZE = 2000\n",
    "\n",
    "if RERUN or not master_df_optimized_path.exists():\n",
    "    rEtaKsstats_dict = dict()\n",
    "\n",
    "    if SKIP_OPTIMIZE_STEP:\n",
    "        master_df['kstest_stat_best'] = master_df['kstest_stat_initial']\n",
    "        master_df[f'best_r'] = master_df['initial_r']\n",
    "        master_df[f'best_eta'] = master_df['initial_eta']\n",
    "    else:\n",
    "        for group in GROUPS: \n",
    "            print(f\"{GROUP.capitalize()} {group}\")\n",
    "            sample = group_data_map[group]\n",
    "            initial_r, initial_eta = master_df.loc[group, 'initial_r'], master_df.loc[group, f'initial_eta']\n",
    "            r_granularity = 10\n",
    "            eta_granularity = 10\n",
    "            for i in range(0, -5, -1):\n",
    "                if tuple([round_to_sigfigs(initial_r + 10.**i, 8), initial_eta]) in all_cdfs:\n",
    "                    r_granularity = 10.**i\n",
    "                if tuple([initial_r, round_to_sigfigs(initial_eta + 10.**i, 8)]) in all_cdfs:\n",
    "                    \n",
    "                    eta_granularity = 10.**i\n",
    "\n",
    "            for d in range(1, NUM_ITERS[group] + 1):\n",
    "                if d == 1:\n",
    "                    initial_r, initial_eta = master_df.loc[group, 'initial_r'], master_df.loc[group, f'initial_eta']\n",
    "                else:\n",
    "                    initial_r, initial_eta = master_df.loc[group, f'iter{d - 1}_r'], master_df.loc[group, f'iter{d - 1}_eta']\n",
    "                r_granularity = r_granularity * 10.0**(-d+1) \n",
    "                eta_granularity = eta_granularity * 10.0**(-d+1) \n",
    "\n",
    "                r_range = [round_to_sigfigs(x) for x in np.arange(initial_r - r_granularity, initial_r + r_granularity, r_granularity/10)]\n",
    "                eta_range = [round_to_sigfigs(x) for x in np.arange(initial_eta - eta_granularity, initial_eta + eta_granularity, eta_granularity/10)]\n",
    "                simple_add_cdfs(r_range, eta_range, dir = os.path.join(CWD, \"groupCDFs\"), folder_name=f'{GROUP}{group}', n_samples = NUM_SAMPLES_OPTIMIZE, debug=False, tail_bound=1e-5)\n",
    "                cdfs_dict = combine_pickles(os.path.join(CWD, \"groupCDFs\", f'{GROUP}{group}'))\n",
    "                \n",
    "\n",
    "                temp_df = create_kurt_var_ksstat_df(cdfs_dict)\n",
    "                temp_df['scale'] = np.clip(master_df.loc[group,'obs_var'] / temp_df['variance'], 0, 1e5)\n",
    "                temp_df['variance'] = temp_df['variance'] * temp_df['scale']\n",
    "                temp_df['kurtosis'] = temp_df['kurtosis'] * temp_df['scale']\n",
    "                ksstats, best_param, kstest_stat = gridsearch(sample, cdfs_dict, debug=True, scales= temp_df['scale'])\n",
    "                temp_df = add_tests_to_df(cdfs_df=temp_df, group=group, var_kurt_df=master_df, ksstats=ksstats)\n",
    "                \n",
    "                group_cdf_df_dict[group] = pd.concat([group_cdf_df_dict[group], temp_df])\n",
    "                master_df.loc[group, 'best_r'], master_df.loc[group, f'best_eta'] = best_param[0], best_param[1]\n",
    "                best_scale = temp_df.loc[(temp_df[\"r\"] ==  best_param[0])&(temp_df[\"eta\"] ==  best_param[1])][\"scale\"].iloc[0]\n",
    "                master_df.loc[group, 'best_scale'] = best_scale\n",
    "                master_df.loc[group, 'kstest_stat_best'] = kstest_stat\n",
    "                master_df.loc[group, f'iter{d}_r'], master_df.loc[group, f'iter{d}_eta'] = best_param[0], best_param[1]\n",
    "                master_df.loc[group, f'kstest_stat_iter{d}'] = kstest_stat\n",
    "                \n",
    "                print(f\"Iter {d} {GROUP} {group} best parameters: {best_param, master_df.loc[group, f'kstest_stat_iter{d}']}\")\n",
    "\n",
    "            temp_df = group_cdf_df_dict[group].sort_values(['r', 'eta'])\n",
    "            rEtaKsstats_dict[group] = [temp_df['r'], temp_df['eta'], temp_df['ksstat']]\n",
    "\n",
    "            eps = 0.1\n",
    "            filtered_df = temp_df[(temp_df['r'] > master_df.loc[group, 'initial_r'] - eps) & \n",
    "                            (temp_df['r'] < master_df.loc[group, 'initial_r'] + eps) &\n",
    "                            (temp_df['eta'] > master_df.loc[group, 'initial_eta'] - eps) &\n",
    "                            (temp_df['eta'] < master_df.loc[group, 'initial_eta'] + eps)]\n",
    "            cols = ['pass_var', 'pass_kstest', 'pass_kurt']\n",
    "            fig = combo_test_plot(filtered_df, cols, \n",
    "                                plot_name=f\"{GROUP.capitalize()} {group} zoomed in: {', '.join([col[5:].capitalize() for col in cols])}\",\n",
    "                                best_param=(master_df.loc[group, 'best_r'], master_df.loc[group, f'best_eta']))\n",
    "            fig.figure.savefig(os.path.join(CWD, \"plots\", f\"optimized_full_grid_search_combo_plot_layer{group}.jpg\"), bbox_inches = 'tight', dpi=600)\n",
    "            \n",
    "    master_df['n_pval_0.05'] = master_df.apply(lambda row : find_n_fixed_pval_stat(row.loc['kstest_stat_best'], row.loc['total_samples']), axis = 1)    \n",
    "    master_df[['total_samples', 'initial_r', 'initial_eta', 'kstest_stat_initial', 'best_r', 'best_eta', 'kstest_stat_best', 'n_pval_0.05']].to_csv(os.path.join(CWD, \"CSVs\", 'optimized_params.csv'))\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", 'master_df_optimized.csv'))\n",
    "    pd.to_pickle(rEtaKsstats_dict, os.path.join(CWD, \"cache\", 'rEtaKsstats_dict.pickle'))\n",
    "\n",
    "master_df = pd.read_csv(master_df_optimized_path, index_col = GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Empirical and Computed CDF/PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in GROUPS:\n",
    "    group_info = master_df.loc[group]\n",
    "    best_r = group_info['best_r']\n",
    "    best_eta = group_info['best_eta']\n",
    "    best_scale = group_info['best_scale']\n",
    "    fig = visualize_cdf_pdf(sample = group_data_map[group], \n",
    "                    params = (best_r, best_eta, best_scale), \n",
    "                    log_scale = True,\n",
    "                    group = group)\n",
    "    fig.savefig(os.path.join(CWD, \"plots\", f'compare_cdf_pdf_layer_{group}.jpg'), bbox_inches = 'tight', dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Gaussian and Laplace Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_func(sample, distro, *args, n_samples=200):\n",
    "    if distro == 'gaussian' or distro == 'normal':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.norm(scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 'laplace':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.laplace(scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 't':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.t(df=2, scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 'prior_r':\n",
    "        eta = args[0]\n",
    "        def r_func(r):\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return r_func\n",
    "    elif distro == 'prior_eta':\n",
    "        r = args[0]\n",
    "        def eta_func(eta):\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return eta_func\n",
    "    elif distro == 'prior':\n",
    "        def r_eta_func(params):\n",
    "            r = params[0]\n",
    "            eta = params[1]\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples, debug=False)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return r_eta_func\n",
    "    elif distro == 'prior_with_scale':\n",
    "        def r_eta_scale_func(params):\n",
    "            r = params[0]\n",
    "            eta = params[1]\n",
    "            scale = params[2]\n",
    "            cdf = compute_prior_cdf(r = r, eta = eta, n_samples=n_samples, debug=False)\n",
    "            return compute_ksstat(sample / np.sqrt(scale), cdf)\n",
    "        return r_eta_scale_func\n",
    "\n",
    "    print(\"Please enter a valid argument for `distro`: 'gaussian', 'laplace', 'prior_r', 'prior_eta', 'prior','prior_with_scale', 't'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_path = Path(os.path.join(CWD, \"CSVs\", 'master_df.csv'))\n",
    "\n",
    "if RERUN or not master_df_path.exists():\n",
    "    upper_bound = int(1e6)\n",
    "    for group in GROUPS:\n",
    "        norm_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 'gaussian'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        laplace_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 'laplace'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        t_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 't'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        \n",
    "        master_df.loc[group, 'param_gaussian'] = round_to_sigfigs(norm_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_gaussian'] = round_to_sigfigs(norm_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_gaussian'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_gaussian']), 6)\n",
    "\n",
    "        master_df.loc[group, 'param_laplace'] = round_to_sigfigs(laplace_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_laplace'] = round_to_sigfigs(laplace_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_laplace'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_laplace']), 6)\n",
    "\n",
    "        master_df.loc[group, 'param_laplace'] = round_to_sigfigs(laplace_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_laplace'] = round_to_sigfigs(laplace_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_laplace'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_laplace']), 6) \n",
    "\n",
    "        master_df.loc[group, 'param_t'] = round_to_sigfigs(t_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_t'] = round_to_sigfigs(t_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_t'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_t']), 6) \n",
    "\n",
    "        master_df.loc[group, 'kstest_pval_gengamma'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_best']))\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", 'master_df.csv'))\n",
    "    \n",
    "master_df = pd.read_csv(os.path.join(CWD, \"CSVs\", 'master_df.csv'), index_col = GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "master_df = pd.read_csv(os.path.join(os.getcwd(), \"CSVs\", 'master_df.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(os.path.join(CWD, \"groupCDFs\"))\n",
    "eng.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbmv_backup2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
